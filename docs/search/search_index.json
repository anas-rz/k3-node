{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to K3 Node","text":"<p><code>k3_node</code> is a library for building multibackend graph neural networks.  Built upon Keras 3.0 the models can be trained using TensorFlow, PyTorch,  or JAX.</p> <p>To install the package, run:</p> <pre><code>git clone https://github.com/anas-rz/k3-node.git # bash\n</code></pre> <pre><code># in your code\nimport sys\nsys.path.append('k3-node')\n\nimport os\nos.environ['KERAS_BACKEND'] = 'tensorflow' # or 'torch' or 'jax'\n\nfrom k3_node import ...\n</code></pre>"},{"location":"#project-structure","title":"Project Structure","text":"<pre><code>k3_node/\n    layers          \n        aggr        \n        conv        \n        norm        \n        pool        \n    models          \n    ops             \n    utils\n</code></pre>"},{"location":"aggr_layers/","title":"Aggregation Layers","text":"<p>             Bases: <code>Aggregation</code></p> <p><code>k3_node.layers.SumAggregation</code> Compute and return the sum of two numbers.</p> <p>Parameters:</p> Name Type Description Default <code>`**kwargs`</code> <p>Additional keyword arguments passed to the <code>Layer</code> superclass.</p> required <p>Returns:</p> Type Description <p>A callable layer instance.</p> Source code in <code>k3_node/layers/aggr/basic.py</code> <pre><code>class SumAggregation(Aggregation):\n    \"\"\"`k3_node.layers.SumAggregation`\n    Compute and return the sum of two numbers.\n\n    Args:\n        `**kwargs`: Additional keyword arguments passed to the `Layer` superclass.\n\n    Returns:\n        A callable layer instance.\n    \"\"\"\n\n    def call(self, x, index=None, axis=-2):\n        return self.reduce(x, index, axis=axis, reduce_fn=ops.segment_sum)\n</code></pre> <p>             Bases: <code>Aggregation</code></p> <p><code>k3_node.layers.MaxAggregation</code> Compute and return the sum of two numbers.</p> <p>Parameters:</p> Name Type Description Default <code>`**kwargs`</code> <p>Additional keyword arguments passed to the <code>Layer</code> superclass.</p> required <p>Returns:</p> Type Description <p>A callable layer instance.</p> Source code in <code>k3_node/layers/aggr/basic.py</code> <pre><code>class MaxAggregation(Aggregation):\n    \"\"\"`k3_node.layers.MaxAggregation`\n    Compute and return the sum of two numbers.\n\n    Args:\n        `**kwargs`: Additional keyword arguments passed to the `Layer` superclass.\n\n    Returns:\n        A callable layer instance.\n    \"\"\"\n    def call(self, x, index=None, axis=-2):\n        return self.reduce(x, index, axis=axis, reduce_fn=ops.segment_max)\n</code></pre> <p>             Bases: <code>Aggregation</code></p> <p><code>k3_node.layers.MeanAggregation</code> Compute and return the sum of two numbers.</p> <p>Parameters:</p> Name Type Description Default <code>`**kwargs`</code> <p>Additional keyword arguments passed to the <code>Layer</code> superclass.</p> required <p>Returns:</p> Type Description <p>A callable layer instance.</p> Source code in <code>k3_node/layers/aggr/basic.py</code> <pre><code>class MeanAggregation(Aggregation):\n    \"\"\"`k3_node.layers.MeanAggregation`\n    Compute and return the sum of two numbers.\n\n    Args:\n        `**kwargs`: Additional keyword arguments passed to the `Layer` superclass.\n\n    Returns:\n        A callable layer instance.\n    \"\"\"\n    def call(self, x, index=None, axis=-2):\n        return self.reduce(x, index, axis=axis, reduce_fn=_segment_mean)\n</code></pre> <p>             Bases: <code>Aggregation</code></p> <p><code>k3_node.layers.SoftmaxAggregation</code> Compute and return the sum of two numbers.</p> <p>Parameters:</p> Name Type Description Default <code>`**kwargs`</code> <p>Additional keyword arguments passed to the <code>Layer</code> superclass.</p> required <p>Returns:</p> Type Description <p>A callable layer instance.</p> Source code in <code>k3_node/layers/aggr/basic.py</code> <pre><code>class SoftmaxAggregation(Aggregation):\n    \"\"\"`k3_node.layers.SoftmaxAggregation`\n    Compute and return the sum of two numbers.\n\n    Args:\n        `**kwargs`: Additional keyword arguments passed to the `Layer` superclass.\n\n    Returns:\n        A callable layer instance.\n    \"\"\"\n    def __init__(self, t=1.0, trainable=False, channels=1):\n        super().__init__()\n\n        if not trainable and channels != 1:\n            raise ValueError(\n                \"Cannot set 'channels' greater than '1' in case 'SoftmaxAggregation' is not trainable\"\n            )\n\n        self._init_t = t\n        self.trainable = trainable\n        self.channels = channels\n\n        self.t = self.add_weight((channels,), initializer=\"zeros\") if trainable else t\n\n    def call(self, x, index=None, axis=-2):\n        t = self.t\n        if self.channels != 1:\n            self.assert_two_dimensional_input(x, axis)\n            assert ops.is_tensor(t)\n            t = ops.reshape(t, (1, self.channels))\n\n        alpha = x\n        if not isinstance(t, (int, float)) or t != 1:\n            alpha = x * t\n        alpha = ops.softmax(alpha, axis=axis)\n        return self.reduce(x * alpha, index=index, axis=axis, reduce_fn=ops.segment_sum)\n</code></pre> <p>             Bases: <code>Aggregation</code></p> <p><code>k3_node.layers.SoftmaxAggregation</code> Compute and return the sum of two numbers.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <p>Additional keyword arguments passed to the <code>Layer</code> superclass.</p> required <p>Returns:</p> Type Description <p>A callable layer instance.</p> Source code in <code>k3_node/layers/aggr/basic.py</code> <pre><code>class PowerMeanAggregation(Aggregation):\n    \"\"\"`k3_node.layers.SoftmaxAggregation`\n    Compute and return the sum of two numbers.\n\n    Args:\n        **kwargs: Additional keyword arguments passed to the `Layer` superclass.\n\n    Returns:\n        A callable layer instance.\n\n    \"\"\"\n    def __init__(self, p=1.0, trainable=False, channels=1):\n        super().__init__()\n\n        if not trainable and channels != 1:\n            raise ValueError(\n                f\"Cannot set 'channels' greater than '1' in case '{self.__class__.__name__}' is not trainable\"\n            )\n\n        self._init_p = p\n        self.trainable = trainable\n        self.channels = channels\n\n        self.p = self.add_weight((channels,), \"zeros\") if trainable else p\n\n    def call(self, x, index=None, axis=-2):\n        p = self.p\n        if self.channels != 1:\n            assert ops.is_tensor(p)\n            self.assert_two_dimensional_input(x, axis)\n            p = ops.reshape(p, (-1, self.channels))\n        if not isinstance(p, (int, float)) or p != 1.0:\n            x = ops.clip(x, 0, 100) ** p\n        out = self.reduce(x, index, axis, reduce_fn=_segment_mean)\n        if not isinstance(p, (int, float)) or p != 1:\n            out = ops.clip(out, 0, 100) ** (1.0 / p)\n        return out\n</code></pre>"},{"location":"attention_layers/","title":"Attention layers","text":"<p>             Bases: <code>Layer</code></p> <p><code>k3_node.layers.PerformerAttention</code></p> <p>Initialization Arguments:</p> <p>Parameters:</p> Name Type Description Default <code>channels</code> <p>The number of output channels.</p> required <code>heads</code> <p>The number of attention heads.</p> required <code>head_channels</code> <p>The number of attention heads.</p> <code>64</code> <code>kernel</code> <p>activation function.</p> <code>relu</code> <code>qkv_bias</code> <p>activation function.</p> <code>False</code> <code>attn_out_bias</code> <p>Bias in Attention Out.</p> <code>True</code> <code>dropout</code> <p>Dropout rate.</p> <code>0.0</code> Source code in <code>k3_node/layers/attention/performer.py</code> <pre><code>class PerformerAttention(layers.Layer):\n    \"\"\"\n    `k3_node.layers.PerformerAttention`\n\n    Initialization Arguments:\n\n    Args:\n        channels: The number of output channels.\n        heads: The number of attention heads.\n        head_channels: The number of attention heads.\n        kernel: activation function.\n        qkv_bias: activation function.\n        attn_out_bias: Bias in Attention Out.\n        dropout: Dropout rate.\n    \"\"\"\n    def __init__(\n        self,\n        channels,\n        heads,\n        head_channels=64,\n        kernel=ops.relu,\n        qkv_bias=False,\n        attn_out_bias=True,\n        dropout=0.0,\n    ):\n        super().__init__()\n        assert channels % heads == 0\n        if head_channels is None:\n            head_channels = channels // heads\n\n        self.heads = heads\n        self.head_channels = head_channels\n        self.kernel = kernel\n        self.fast_attn = PerformerProjection(head_channels, kernel)\n\n        inner_channels = head_channels * heads\n        self.q = layers.Dense(inner_channels, use_bias=qkv_bias)\n        self.k = layers.Dense(inner_channels, use_bias=qkv_bias)\n        self.v = layers.Dense(inner_channels, use_bias=qkv_bias)\n        self.attn_out = layers.Dense(channels, use_bias=attn_out_bias)\n        self.dropout = layers.Dropout(dropout)\n\n    def call(self, x, mask=None):\n        B, N, *_ = x.shape\n        q, k, v = self.q(x), self.k(x), self.v(x)\n\n        q = ops.transpose(\n            ops.reshape(q, (B, N, self.heads, self.head_channels)), axes=(0, 2, 1, 3)\n        )\n        k = ops.transpose(\n            ops.reshape(k, (B, N, self.heads, self.head_channels)), axes=(0, 2, 1, 3)\n        )\n        v = ops.transpose(\n            ops.reshape(v, (B, N, self.heads, self.head_channels)), axes=(0, 2, 1, 3)\n        )\n\n        if mask is not None:\n            mask = mask[:, None, :, None]\n            v = ops.where(mask, v, ops.zeros_like(v))\n\n        out = self.fast_attn(q, k, v)\n        out = ops.transpose(out, axes=(0, 2, 1, 3))  # Transpose back\n        out = ops.reshape(out, (B, N, -1))\n        out = self.attn_out(out)\n        out = self.dropout(out)\n        return out\n</code></pre>"},{"location":"conv_layers/","title":"Convolution Layers","text":"<p>             Bases: <code>MessagePassing</code></p> <p><code>k3_node.layers.AGNNConv</code>  Implementation of Attention-based Graph Neural Network (AGNN) layer</p> <p>Parameters:</p> Name Type Description Default <code>trainable</code> <p>Whether to learn the scaling factor beta.</p> <code>True</code> <code>aggregate</code> <p>Aggregation function to use (one of 'sum', 'mean', 'max').</p> <code>'sum'</code> <code>activation</code> <p>Activation function to use.</p> <code>None</code> <code>**kwargs</code> <p>Additional arguments to pass to the <code>MessagePassing</code> superclass.</p> <code>{}</code> Source code in <code>k3_node/layers/conv/agnn_conv.py</code> <pre><code>class AGNNConv(MessagePassing):\n    \"\"\"\n    `k3_node.layers.AGNNConv` \n    Implementation of Attention-based Graph Neural Network (AGNN) layer\n\n    Args:\n        trainable: Whether to learn the scaling factor beta.\n        aggregate: Aggregation function to use (one of 'sum', 'mean', 'max').\n        activation: Activation function to use.\n        **kwargs: Additional arguments to pass to the `MessagePassing` superclass.\n    \"\"\"\n    def __init__(self, trainable=True, aggregate=\"sum\", activation=None, **kwargs):\n        super().__init__(aggregate=aggregate, activation=activation, **kwargs)\n        self.trainable = trainable\n\n    def build(self, input_shape):\n        assert len(input_shape) &gt;= 2\n        if self.trainable:\n            self.beta = self.add_weight(shape=(1,), initializer=\"ones\", name=\"beta\")\n        else:\n            self.beta = ops.cast(1.0, self.dtype)\n        self.built = True\n\n    def call(self, x, a, **kwargs):\n        x_norm = keras.utils.normalize(x, axis=-1)\n        output = self.propagate(x, a, x_norm=x_norm)\n        output = self.activation(output)\n\n        return output\n\n    def message(self, x, x_norm=None):\n        x_j = self.get_sources(x)\n        x_norm_i = self.get_targets(x_norm)\n        x_norm_j = self.get_sources(x_norm)\n        alpha = self.beta * ops.sum(x_norm_i * x_norm_j, axis=-1)\n\n        if len(alpha.shape) == 2:\n            alpha = ops.transpose(alpha)  # For mixed mode\n        alpha = segment_softmax(alpha, self.index_targets, self.n_nodes)\n        if len(alpha.shape) == 2:\n            alpha = ops.transpose(alpha)  # For mixed mode\n        alpha = alpha[..., None]\n\n        return alpha * x_j\n\n    @property\n    def config(self):\n        return {\n            \"trainable\": self.trainable,\n        }\n</code></pre> <p>             Bases: <code>Conv</code></p> <p><code>k3_node.layers.APPNPConv</code> Implementation of Approximate Personalized Propagation of Neural Predictions</p> <p>Parameters:</p> Name Type Description Default <code>channels</code> <p>The number of output channels.</p> required <code>alpha</code> <p>The teleport probability.</p> <code>0.2</code> <code>propagations</code> <p>The number of propagation steps.</p> <code>1</code> <code>mlp_hidden</code> <p>A list of hidden channels for the MLP.</p> <code>None</code> <code>mlp_activation</code> <p>The activation function to use in the MLP.</p> <code>'relu'</code> <code>dropout_rate</code> <p>The dropout rate for the MLP.</p> <code>0.0</code> <code>activation</code> <p>The activation function to use in the layer.</p> <code>None</code> <code>use_bias</code> <p>Whether to add a bias to the linear transformation.</p> <code>True</code> <code>kernel_initializer</code> <p>Initializer for the <code>kernel</code> weights matrix.</p> <code>'glorot_uniform'</code> <code>bias_initializer</code> <p>Initializer for the bias vector.</p> <code>'zeros'</code> <code>kernel_regularizer</code> <p>Regularizer for the <code>kernel</code> weights matrix.</p> <code>None</code> <code>bias_regularizer</code> <p>Regularizer for the bias vector.</p> <code>None</code> <code>activity_regularizer</code> <p>Regularizer for the output.</p> <code>None</code> <code>kernel_constraint</code> <p>Constraint for the <code>kernel</code> weights matrix.</p> <code>None</code> <code>bias_constraint</code> <p>Constraint for the bias vector.</p> <code>None</code> <code>**kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code> Source code in <code>k3_node/layers/conv/appnp_conv.py</code> <pre><code>class APPNPConv(Conv):\n    \"\"\"\n        `k3_node.layers.APPNPConv`\n        Implementation of Approximate Personalized Propagation of Neural Predictions\n\n        Args:\n            channels: The number of output channels.\n            alpha: The teleport probability.\n            propagations: The number of propagation steps.\n            mlp_hidden: A list of hidden channels for the MLP.\n            mlp_activation: The activation function to use in the MLP.\n            dropout_rate: The dropout rate for the MLP.\n            activation: The activation function to use in the layer.\n            use_bias: Whether to add a bias to the linear transformation.\n            kernel_initializer: Initializer for the `kernel` weights matrix.\n            bias_initializer: Initializer for the bias vector.\n            kernel_regularizer: Regularizer for the `kernel` weights matrix.\n            bias_regularizer: Regularizer for the bias vector.\n            activity_regularizer: Regularizer for the output.\n            kernel_constraint: Constraint for the `kernel` weights matrix.\n            bias_constraint: Constraint for the bias vector.\n            **kwargs: Additional keyword arguments.\n    \"\"\"\n    def __init__(\n        self,\n        channels,\n        alpha=0.2,\n        propagations=1,\n        mlp_hidden=None,\n        mlp_activation=\"relu\",\n        dropout_rate=0.0,\n        activation=None,\n        use_bias=True,\n        kernel_initializer=\"glorot_uniform\",\n        bias_initializer=\"zeros\",\n        kernel_regularizer=None,\n        bias_regularizer=None,\n        activity_regularizer=None,\n        kernel_constraint=None,\n        bias_constraint=None,\n        **kwargs,\n    ):\n\n        super().__init__(\n            activation=activation,\n            use_bias=use_bias,\n            kernel_initializer=kernel_initializer,\n            bias_initializer=bias_initializer,\n            kernel_regularizer=kernel_regularizer,\n            bias_regularizer=bias_regularizer,\n            activity_regularizer=activity_regularizer,\n            kernel_constraint=kernel_constraint,\n            bias_constraint=bias_constraint,\n            **kwargs,\n        )\n        self.channels = channels\n        self.mlp_hidden = mlp_hidden if mlp_hidden else []\n        self.alpha = alpha\n        self.propagations = propagations\n        self.mlp_activation = activations.get(mlp_activation)\n        self.dropout_rate = dropout_rate\n\n    def build(self, input_shape):\n        assert len(input_shape) &gt;= 2\n        layer_kwargs = dict(\n            kernel_initializer=self.kernel_initializer,\n            bias_initializer=self.bias_initializer,\n            kernel_regularizer=self.kernel_regularizer,\n            bias_regularizer=self.bias_regularizer,\n            kernel_constraint=self.kernel_constraint,\n            bias_constraint=self.bias_constraint,\n            dtype=self.dtype,\n        )\n        mlp_layers = []\n        for channels in self.mlp_hidden:\n            mlp_layers.extend(\n                [\n                    Dropout(self.dropout_rate),\n                    Dense(channels, self.mlp_activation, **layer_kwargs),\n                ]\n            )\n        mlp_layers.append(Dense(self.channels, \"linear\", **layer_kwargs))\n        self.mlp = Sequential(mlp_layers)\n        self.built = True\n\n    def call(self, inputs, mask=None):\n        x, a = inputs\n        mlp_out = self.mlp(x)\n        output = mlp_out\n        for _ in range(self.propagations):\n            output = (1 - self.alpha) * modal_dot(a, output) + self.alpha * mlp_out\n        if mask[0] is not None:\n            output *= mask[0]\n        output = self.activation(output)\n\n        return output\n\n    @property\n    def config(self):\n        return {\n            \"channels\": self.channels,\n            \"alpha\": self.alpha,\n            \"propagations\": self.propagations,\n            \"mlp_hidden\": self.mlp_hidden,\n            \"mlp_activation\": activations.serialize(self.mlp_activation),\n            \"dropout_rate\": self.dropout_rate,\n        }\n\n    @staticmethod\n    def preprocess(a):\n        return gcn_filter(a)\n</code></pre> <p>             Bases: <code>Conv</code></p> <p><code>k3_node.layers.ARMAConv</code>  Implementation of ARMAConv layer</p> <p>Parameters:</p> Name Type Description Default <code>channels</code> <p>The number of output channels.</p> required <code>order</code> <p>The order of the ARMA filter.</p> <code>1</code> <code>iterations</code> <p>The number of iterations to perform.</p> <code>1</code> <code>share_weights</code> <p>Whether to share the weights across iterations.</p> <code>False</code> <code>gcn_activation</code> <p>The activation function to use for GCN.</p> <code>'relu'</code> <code>dropout_rate</code> <p>The dropout rate.</p> <code>0.0</code> <code>activation</code> <p>The activation function to use in the layer.</p> <code>None</code> <code>use_bias</code> <p>Whether to add a bias to the linear transformation.</p> <code>True</code> <code>kernel_initializer</code> <p>Initializer for the <code>kernel</code> weights matrix.</p> <code>'glorot_uniform'</code> <code>bias_initializer</code> <p>Initializer for the bias vector.</p> <code>'zeros'</code> <code>kernel_regularizer</code> <p>Regularizer for the <code>kernel</code> weights matrix.</p> <code>None</code> <code>bias_regularizer</code> <p>Regularizer for the bias vector.</p> <code>None</code> <code>activity_regularizer</code> <p>Regularizer for the output.</p> <code>None</code> <code>kernel_constraint</code> <p>Constraint for the <code>kernel</code> weights matrix.</p> <code>None</code> <code>bias_constraint</code> <p>Constraint for the bias vector.</p> <code>None</code> <code>**kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code> Source code in <code>k3_node/layers/conv/arma_conv.py</code> <pre><code>class ARMAConv(Conv):\n    \"\"\"\n    `k3_node.layers.ARMAConv` \n    Implementation of ARMAConv layer\n\n    Args:\n        channels: The number of output channels.\n        order: The order of the ARMA filter.\n        iterations: The number of iterations to perform.\n        share_weights: Whether to share the weights across iterations.\n        gcn_activation: The activation function to use for GCN.\n        dropout_rate: The dropout rate.\n        activation: The activation function to use in the layer.\n        use_bias: Whether to add a bias to the linear transformation.\n        kernel_initializer: Initializer for the `kernel` weights matrix.\n        bias_initializer: Initializer for the bias vector.\n        kernel_regularizer: Regularizer for the `kernel` weights matrix.\n        bias_regularizer: Regularizer for the bias vector.\n        activity_regularizer: Regularizer for the output.\n        kernel_constraint: Constraint for the `kernel` weights matrix.\n        bias_constraint: Constraint for the bias vector.\n        **kwargs: Additional keyword arguments.\n    \"\"\"\n    def __init__(\n        self,\n        channels,\n        order=1,\n        iterations=1,\n        share_weights=False,\n        gcn_activation=\"relu\",\n        dropout_rate=0.0,\n        activation=None,\n        use_bias=True,\n        kernel_initializer=\"glorot_uniform\",\n        bias_initializer=\"zeros\",\n        kernel_regularizer=None,\n        bias_regularizer=None,\n        activity_regularizer=None,\n        kernel_constraint=None,\n        bias_constraint=None,\n        **kwargs,\n    ):\n\n        super().__init__(\n            activation=activation,\n            use_bias=use_bias,\n            kernel_initializer=kernel_initializer,\n            bias_initializer=bias_initializer,\n            kernel_regularizer=kernel_regularizer,\n            bias_regularizer=bias_regularizer,\n            activity_regularizer=activity_regularizer,\n            kernel_constraint=kernel_constraint,\n            bias_constraint=bias_constraint,\n            **kwargs,\n        )\n        self.channels = channels\n        self.iterations = iterations\n        self.order = order\n        self.share_weights = share_weights\n        self.gcn_activation = activations.get(gcn_activation)\n        self.dropout_rate = dropout_rate\n\n    def build(self, input_shape):\n        assert len(input_shape) &gt;= 2\n        F = input_shape[0][-1]\n\n        # Create weights for parallel stacks\n        # self.kernels[k][i] refers to the k-th stack, i-th iteration\n        self.kernels = []\n        for k in range(self.order):\n            kernel_stack = []\n            current_shape = F\n            for i in range(self.iterations):\n                kernel_stack.append(\n                    self.create_weights(\n                        current_shape, F, self.channels, \"ARMA_GCS_{}{}\".format(k, i)\n                    )\n                )\n                current_shape = self.channels\n                if self.share_weights and i == 1:\n                    # No need to continue because all weights will be shared\n                    break\n            self.kernels.append(kernel_stack)\n\n        self.dropout = Dropout(self.dropout_rate, dtype=self.dtype)\n        self.built = True\n\n    def call(self, inputs, mask=None):\n        x, a = inputs\n\n        output = []\n        for k in range(self.order):\n            output_k = x\n            for i in range(self.iterations):\n                output_k = self.gcs([output_k, x, a], k, i)\n            output.append(output_k)\n        output = ops.stack(output, axis=-1)\n        output = ops.mean(output, axis=-1)\n\n        if mask[0] is not None:\n            output *= mask[0]\n        output = self.activation(output)\n\n        return output\n\n    def create_weights(self, input_dim, input_dim_skip, channels, name):\n        kernel_1 = self.add_weight(\n            shape=(input_dim, channels),\n            name=name + \"_kernel_1\",\n            initializer=self.kernel_initializer,\n            regularizer=self.kernel_regularizer,\n            constraint=self.kernel_constraint,\n        )\n        kernel_2 = self.add_weight(\n            shape=(input_dim_skip, channels),\n            name=name + \"_kernel_2\",\n            initializer=self.kernel_initializer,\n            regularizer=self.kernel_regularizer,\n            constraint=self.kernel_constraint,\n        )\n        bias = None\n        if self.use_bias:\n            bias = self.add_weight(\n                shape=(channels,),\n                name=name + \"_bias\",\n                initializer=self.bias_initializer,\n                regularizer=self.bias_regularizer,\n                constraint=self.bias_constraint,\n            )\n        return kernel_1, kernel_2, bias\n\n    def gcs(self, inputs, stack, iteration):\n        x, x_skip, a = inputs\n\n        itr = 1 if self.share_weights and iteration &gt;= 1 else iteration\n        kernel_1, kernel_2, bias = self.kernels[stack][itr]\n\n        output = ops.dot(x, kernel_1)\n        output = modal_dot(a, output)\n\n        skip = ops.dot(x_skip, kernel_2)\n        skip = self.dropout(skip)\n        output += skip\n\n        if self.use_bias:\n            output = ops.add(output, bias)\n        output = self.gcn_activation(output)\n\n        return output\n\n    @property\n    def config(self):\n        return {\n            \"channels\": self.channels,\n            \"iterations\": self.iterations,\n            \"order\": self.order,\n            \"share_weights\": self.share_weights,\n            \"gcn_activation\": activations.serialize(self.gcn_activation),\n            \"dropout_rate\": self.dropout_rate,\n        }\n\n    @staticmethod\n    def preprocess(a):\n        return normalized_adjacency(a, symmetric=True)\n</code></pre> <p>             Bases: <code>MessagePassing</code></p> <p><code>k3_node.layers.CrystalConv</code> Implementation of Crystal Graph Convolutional Neural Networks (CGCNN) layer</p> <p>Parameters:</p> Name Type Description Default <code>aggregate</code> <p>Aggregation function to use (one of 'sum', 'mean', 'max').</p> <code>'sum'</code> <code>activation</code> <p>Activation function to use.</p> <code>None</code> <code>use_bias</code> <p>Whether to add a bias to the linear transformation.</p> <code>True</code> <code>kernel_initializer</code> <p>Initializer for the <code>kernel</code> weights matrix.</p> <code>'glorot_uniform'</code> <code>bias_initializer</code> <p>Initializer for the bias vector.</p> <code>'zeros'</code> <code>kernel_regularizer</code> <p>Regularizer for the <code>kernel</code> weights matrix.</p> <code>None</code> <code>bias_regularizer</code> <p>Regularizer for the bias vector.</p> <code>None</code> <code>activity_regularizer</code> <p>Regularizer for the output.</p> <code>None</code> <code>kernel_constraint</code> <p>Constraint for the <code>kernel</code> weights matrix.</p> <code>None</code> <code>bias_constraint</code> <p>Constraint for the bias vector.</p> <code>None</code> <code>**kwargs</code> <p>Additional arguments to pass to the <code>MessagePassing</code> superclass.</p> <code>{}</code> Source code in <code>k3_node/layers/conv/crystal_conv.py</code> <pre><code>class CrystalConv(MessagePassing):\n    \"\"\"\n    `k3_node.layers.CrystalConv`\n    Implementation of Crystal Graph Convolutional Neural Networks (CGCNN) layer\n\n    Args:\n        aggregate: Aggregation function to use (one of 'sum', 'mean', 'max').\n        activation: Activation function to use.\n        use_bias: Whether to add a bias to the linear transformation.\n        kernel_initializer: Initializer for the `kernel` weights matrix.\n        bias_initializer: Initializer for the bias vector.\n        kernel_regularizer: Regularizer for the `kernel` weights matrix.\n        bias_regularizer: Regularizer for the bias vector.\n        activity_regularizer: Regularizer for the output.\n        kernel_constraint: Constraint for the `kernel` weights matrix.\n        bias_constraint: Constraint for the bias vector.\n        **kwargs: Additional arguments to pass to the `MessagePassing` superclass. \n    \"\"\"\n    def __init__(\n        self,\n        aggregate=\"sum\",\n        activation=None,\n        use_bias=True,\n        kernel_initializer=\"glorot_uniform\",\n        bias_initializer=\"zeros\",\n        kernel_regularizer=None,\n        bias_regularizer=None,\n        activity_regularizer=None,\n        kernel_constraint=None,\n        bias_constraint=None,\n        **kwargs,\n    ):\n\n        super().__init__(\n            aggregate=aggregate,\n            activation=activation,\n            use_bias=use_bias,\n            kernel_initializer=kernel_initializer,\n            bias_initializer=bias_initializer,\n            kernel_regularizer=kernel_regularizer,\n            bias_regularizer=bias_regularizer,\n            activity_regularizer=activity_regularizer,\n            kernel_constraint=kernel_constraint,\n            bias_constraint=bias_constraint,\n            **kwargs,\n        )\n\n    def build(self, input_shape):\n        assert len(input_shape) &gt;= 2\n        layer_kwargs = dict(\n            kernel_initializer=self.kernel_initializer,\n            bias_initializer=self.bias_initializer,\n            kernel_regularizer=self.kernel_regularizer,\n            bias_regularizer=self.bias_regularizer,\n            kernel_constraint=self.kernel_constraint,\n            bias_constraint=self.bias_constraint,\n            dtype=self.dtype,\n        )\n        channels = input_shape[0][-1]\n        self.dense_f = Dense(channels, activation=\"sigmoid\", **layer_kwargs)\n        self.dense_s = Dense(channels, activation=self.activation, **layer_kwargs)\n\n        self.built = True\n\n    def message(self, x, e=None):\n        x_i = self.get_targets(x)\n        x_j = self.get_sources(x)\n\n        to_concat = [x_i, x_j]\n        if e is not None:\n            to_concat += [e]\n        z = ops.concatenate(to_concat, axis=-1)\n        output = self.dense_s(z) * self.dense_f(z)\n\n        return output\n\n    def update(self, embeddings, x=None):\n        return x + embeddings\n</code></pre> <p>             Bases: <code>Conv</code></p> <p><code>k3_node.layers.DiffusionConv</code> Implementation of Diffusion Convolutional Neural Networks (DCNN) layer</p> <p>Parameters:</p> Name Type Description Default <code>channels</code> <p>The number of output channels.</p> required <code>K</code> <p>The number of diffusion steps.</p> <code>6</code> <code>activation</code> <p>Activation function to use.</p> <code>'tanh'</code> <code>kernel_initializer</code> <p>Initializer for the <code>kernel</code> weights matrix.</p> <code>'glorot_uniform'</code> <code>kernel_regularizer</code> <p>Regularizer for the <code>kernel</code> weights matrix.</p> <code>None</code> <code>kernel_constraint</code> <p>Constraint for the <code>kernel</code> weights matrix.</p> <code>None</code> <code>**kwargs</code> <p>Additional arguments to pass to the <code>Conv</code> superclass.</p> <code>{}</code> Source code in <code>k3_node/layers/conv/diffusion_conv.py</code> <pre><code>class DiffusionConv(Conv):\n    \"\"\"\n    `k3_node.layers.DiffusionConv`\n    Implementation of Diffusion Convolutional Neural Networks (DCNN) layer\n\n    Args:\n        channels: The number of output channels.\n        K: The number of diffusion steps.\n        activation: Activation function to use.\n        kernel_initializer: Initializer for the `kernel` weights matrix.\n        kernel_regularizer: Regularizer for the `kernel` weights matrix.\n        kernel_constraint: Constraint for the `kernel` weights matrix.\n        **kwargs: Additional arguments to pass to the `Conv` superclass.\n    \"\"\"\n    def __init__(\n        self,\n        channels,\n        K=6,\n        activation=\"tanh\",\n        kernel_initializer=\"glorot_uniform\",\n        kernel_regularizer=None,\n        kernel_constraint=None,\n        **kwargs,\n    ):\n        super().__init__(\n            activation=activation,\n            kernel_initializer=kernel_initializer,\n            kernel_regularizer=kernel_regularizer,\n            kernel_constraint=kernel_constraint,\n            **kwargs,\n        )\n\n        self.channels = channels\n        self.K = K + 1\n\n    def build(self, input_shape):\n        self.filters = [\n            DiffuseFeatures(\n                num_diffusion_steps=self.K,\n                kernel_initializer=self.kernel_initializer,\n                kernel_regularizer=self.kernel_regularizer,\n                kernel_constraint=self.kernel_constraint,\n            )\n            for _ in range(self.channels)\n        ]\n\n    def apply_filters(self, x, a):\n        diffused_features = []\n\n        for diffusion in self.filters:\n            diffused_feature = diffusion((x, a))\n            diffused_features.append(diffused_feature)\n\n        return ops.concatenate(diffused_features, -1)\n\n    def call(self, inputs):\n        x, a = inputs\n        output = self.apply_filters(x, a)\n\n        output = self.activation(output)\n\n        return output\n\n    @property\n    def config(self):\n        return {\"channels\": self.channels, \"K\": self.K - 1}\n\n    @staticmethod\n    def preprocess(a):\n        return normalized_adjacency(a)\n</code></pre> <p>             Bases: <code>MessagePassing</code></p> <p><code>k3_node.layers.GatedGraphConv</code> </p> <p>Implementation of Gated Graph Convolution (GGC) layer</p> <p>Parameters:</p> Name Type Description Default <code>channels</code> <p>The number of output channels.</p> required <code>n_layers</code> <p>The number of GGC layers to stack.</p> required <code>activation</code> <p>Activation function to use.</p> <code>None</code> <code>use_bias</code> <p>Whether to add a bias to the linear transformation.</p> <code>True</code> <code>kernel_initializer</code> <p>Initializer for the <code>kernel</code> weights matrix.</p> <code>'glorot_uniform'</code> <code>bias_initializer</code> <p>Initializer for the bias vector.</p> <code>'zeros'</code> <code>kernel_regularizer</code> <p>Regularizer for the <code>kernel</code> weights matrix.</p> <code>None</code> <code>bias_regularizer</code> <p>Regularizer for the bias vector.</p> <code>None</code> <code>activity_regularizer</code> <p>Regularizer for the output.</p> <code>None</code> <code>kernel_constraint</code> <p>Constraint for the <code>kernel</code> weights matrix.</p> <code>None</code> <code>bias_constraint</code> <p>Constraint for the bias vector.</p> <code>None</code> <code>**kwargs</code> <p>Additional arguments to pass to the <code>MessagePassing</code> superclass.</p> <code>{}</code> Source code in <code>k3_node/layers/conv/gated_graph_conv.py</code> <pre><code>class GatedGraphConv(MessagePassing):\n    \"\"\"\n    `k3_node.layers.GatedGraphConv` \n\n    Implementation of Gated Graph Convolution (GGC) layer\n\n    Args:\n        channels: The number of output channels.\n        n_layers: The number of GGC layers to stack.\n        activation: Activation function to use.\n        use_bias: Whether to add a bias to the linear transformation.\n        kernel_initializer: Initializer for the `kernel` weights matrix.\n        bias_initializer: Initializer for the bias vector.\n        kernel_regularizer: Regularizer for the `kernel` weights matrix.\n        bias_regularizer: Regularizer for the bias vector.\n        activity_regularizer: Regularizer for the output.\n        kernel_constraint: Constraint for the `kernel` weights matrix.\n        bias_constraint: Constraint for the bias vector.\n        **kwargs: Additional arguments to pass to the `MessagePassing` superclass.\n\n    \"\"\"\n    def __init__(\n        self,\n        channels,\n        n_layers,\n        activation=None,\n        use_bias=True,\n        kernel_initializer=\"glorot_uniform\",\n        bias_initializer=\"zeros\",\n        kernel_regularizer=None,\n        bias_regularizer=None,\n        activity_regularizer=None,\n        kernel_constraint=None,\n        bias_constraint=None,\n        **kwargs,\n    ):\n        super().__init__(\n            activation=activation,\n            use_bias=use_bias,\n            kernel_initializer=kernel_initializer,\n            bias_initializer=bias_initializer,\n            kernel_regularizer=kernel_regularizer,\n            bias_regularizer=bias_regularizer,\n            activity_regularizer=activity_regularizer,\n            kernel_constraint=kernel_constraint,\n            bias_constraint=bias_constraint,\n            **kwargs,\n        )\n        self.channels = channels\n        self.n_layers = n_layers\n\n    def build(self, input_shape):\n        assert len(input_shape) &gt;= 2\n        self.kernel = self.add_weight(\n            name=\"kernel\",\n            shape=(self.n_layers, self.channels, self.channels),\n            initializer=self.kernel_initializer,\n            regularizer=self.kernel_regularizer,\n            constraint=self.kernel_constraint,\n        )\n        self.rnn = GRUCell(\n            self.channels,\n            kernel_initializer=self.kernel_initializer,\n            bias_initializer=self.bias_initializer,\n            kernel_regularizer=self.kernel_regularizer,\n            bias_regularizer=self.bias_regularizer,\n            activity_regularizer=self.activity_regularizer,\n            kernel_constraint=self.kernel_constraint,\n            bias_constraint=self.bias_constraint,\n            use_bias=self.use_bias,\n            dtype=self.dtype,\n        )\n        self.built = True\n\n    def call(self, inputs):\n        x, a, _ = self.get_inputs(inputs)\n        F = ops.shape(x)[-1]\n        assert F &lt;= self.channels\n        to_pad = self.channels - F\n        ndims = len(x.shape) - 1\n        output = ops.pad(x, [[0, 0]] * ndims + [[0, to_pad]])\n        for i in range(self.n_layers):\n            m = ops.matmul(output, self.kernel[i])\n            m = self.propagate(m, a)\n            output = self.rnn(m, [output])[0]\n\n        output = self.activation(output)\n        return output\n\n    @property\n    def config(self):\n        return {\n            \"channels\": self.channels,\n            \"n_layers\": self.n_layers,\n        }\n</code></pre> <p>             Bases: <code>Layer</code></p> <p><code>k3_node.layers.GraphConvolution</code>  Implementation of Graph Convolution (GCN) layer</p> <p>Parameters:</p> Name Type Description Default <code>units</code> <p>Positive integer, dimensionality of the output space.</p> required <code>activation</code> <p>Activation function to use.</p> <code>None</code> <code>use_bias</code> <p>Whether to add a bias to the linear transformation.</p> <code>True</code> <code>final_layer</code> <p>Deprecated, use tf.gather or GatherIndices instead.</p> <code>None</code> <code>input_dim</code> <p>Deprecated, use <code>keras.layers.Input</code> with <code>input_shape</code> instead.</p> <code>None</code> <code>kernel_initializer</code> <p>Initializer for the <code>kernel</code> weights matrix.</p> <code>'glorot_uniform'</code> <code>kernel_regularizer</code> <p>Regularizer for the <code>kernel</code> weights matrix.</p> <code>None</code> <code>kernel_constraint</code> <p>Constraint for the <code>kernel</code> weights matrix.</p> <code>None</code> <code>bias_initializer</code> <p>Initializer for the bias vector.</p> <code>'zeros'</code> <code>bias_regularizer</code> <p>Regularizer for the bias vector.</p> <code>None</code> <code>bias_constraint</code> <p>Constraint for the bias vector.</p> <code>None</code> <code>**kwargs</code> <p>Additional arguments to pass to the <code>Layer</code> superclass.</p> <code>{}</code> Source code in <code>k3_node/layers/conv/gcn.py</code> <pre><code>class GraphConvolution(Layer):\n    \"\"\"\n    `k3_node.layers.GraphConvolution` \n    Implementation of Graph Convolution (GCN) layer\n\n    Args:\n        units: Positive integer, dimensionality of the output space.\n        activation: Activation function to use.\n        use_bias: Whether to add a bias to the linear transformation.\n        final_layer: Deprecated, use tf.gather or GatherIndices instead.\n        input_dim: Deprecated, use `keras.layers.Input` with `input_shape` instead.\n        kernel_initializer: Initializer for the `kernel` weights matrix.\n        kernel_regularizer: Regularizer for the `kernel` weights matrix.\n        kernel_constraint: Constraint for the `kernel` weights matrix.\n        bias_initializer: Initializer for the bias vector.\n        bias_regularizer: Regularizer for the bias vector.\n        bias_constraint: Constraint for the bias vector.\n        **kwargs: Additional arguments to pass to the `Layer` superclass.\n    \"\"\"\n    def __init__(\n        self,\n        units,\n        activation=None,\n        use_bias=True,\n        final_layer=None,\n        input_dim=None,\n        kernel_initializer=\"glorot_uniform\",\n        kernel_regularizer=None,\n        kernel_constraint=None,\n        bias_initializer=\"zeros\",\n        bias_regularizer=None,\n        bias_constraint=None,\n        **kwargs,\n    ):\n        if \"input_shape\" not in kwargs and input_dim is not None:\n            kwargs[\"input_shape\"] = (input_dim,)\n\n        self.units = units\n        self.activation = activations.get(activation)\n        self.use_bias = use_bias\n        if final_layer is not None:\n            raise ValueError(\n                \"'final_layer' is not longer supported, use 'tf.gather' or 'GatherIndices' separately\"\n            )\n\n        self.kernel_initializer = initializers.get(kernel_initializer)\n        self.kernel_regularizer = regularizers.get(kernel_regularizer)\n        self.kernel_constraint = constraints.get(kernel_constraint)\n        self.bias_initializer = initializers.get(bias_initializer)\n        self.bias_regularizer = regularizers.get(bias_regularizer)\n        self.bias_constraint = constraints.get(bias_constraint)\n\n        super().__init__(**kwargs)\n\n    def build(self, input_shapes):\n        feat_shape = input_shapes[0]\n        input_dim = int(feat_shape[-1])\n\n        self.kernel = self.add_weight(\n            shape=(1, input_dim, self.units),\n            initializer=self.kernel_initializer,\n            name=\"kernel\",\n            regularizer=self.kernel_regularizer,\n            constraint=self.kernel_constraint,\n        )\n\n        if self.use_bias:\n            self.bias = self.add_weight(\n                shape=(self.units,),\n                initializer=self.bias_initializer,\n                name=\"bias\",\n                regularizer=self.bias_regularizer,\n                constraint=self.bias_constraint,\n            )\n        else:\n            self.bias = None\n        self.built = True\n\n    def call(self, inputs):\n        features, A = inputs\n\n        # Calculate the layer operation of GCN\n\n        h_graph = dot((A, features), axes=1)\n        b = ops.shape(h_graph)[0]\n        kernel = ops.repeat(self.kernel, b, axis=0)\n        output = dot((h_graph, kernel), axes=(-1, 1))\n\n        # Add optional bias &amp; apply activation\n        if self.bias is not None:\n            output += self.bias\n        output = self.activation(output)\n\n        return output\n</code></pre> <p>             Bases: <code>MessagePassing</code></p> <p><code>k3_node.layers.GeneralConv</code> Implementation of General Graph Convolution</p> <p>Parameters:</p> Name Type Description Default <code>channels</code> <p>The number of output channels.</p> <code>256</code> <code>batch_norm</code> <p>Whether to use batch normalization.</p> <code>True</code> <code>dropout</code> <p>The dropout rate.</p> <code>0.0</code> <code>aggregate</code> <p>Aggregation function to use (one of 'sum', 'mean', 'max').</p> <code>'sum'</code> <code>activation</code> <p>Activation function to use.</p> <code>'prelu'</code> <code>use_bias</code> <p>Whether to add a bias to the linear transformation.</p> <code>True</code> <code>kernel_initializer</code> <p>Initializer for the <code>kernel</code> weights matrix.</p> <code>'glorot_uniform'</code> <code>bias_initializer</code> <p>Initializer for the bias vector.</p> <code>'zeros'</code> <code>kernel_regularizer</code> <p>Regularizer for the <code>kernel</code> weights matrix.</p> <code>None</code> <code>bias_regularizer</code> <p>Regularizer for the bias vector.</p> <code>None</code> <code>activity_regularizer</code> <p>Regularizer for the output.</p> <code>None</code> <code>kernel_constraint</code> <p>Constraint for the <code>kernel</code> weights matrix.</p> <code>None</code> <code>bias_constraint</code> <p>Constraint for the bias vector.</p> <code>None</code> <code>**kwargs</code> <p>Additional arguments to pass to the <code>MessagePassing</code> superclass.</p> <code>{}</code> Source code in <code>k3_node/layers/conv/general_conv.py</code> <pre><code>class GeneralConv(MessagePassing):\n    \"\"\"\n    `k3_node.layers.GeneralConv`\n    Implementation of General Graph Convolution\n\n    Args:\n        channels: The number of output channels.\n        batch_norm: Whether to use batch normalization.\n        dropout: The dropout rate.\n        aggregate: Aggregation function to use (one of 'sum', 'mean', 'max').\n        activation: Activation function to use.\n        use_bias: Whether to add a bias to the linear transformation.\n        kernel_initializer: Initializer for the `kernel` weights matrix.\n        bias_initializer: Initializer for the bias vector.\n        kernel_regularizer: Regularizer for the `kernel` weights matrix.\n        bias_regularizer: Regularizer for the bias vector.\n        activity_regularizer: Regularizer for the output.\n        kernel_constraint: Constraint for the `kernel` weights matrix.\n        bias_constraint: Constraint for the bias vector.\n        **kwargs: Additional arguments to pass to the `MessagePassing` superclass. \n    \"\"\"\n    def __init__(\n        self,\n        channels=256,\n        batch_norm=True,\n        dropout=0.0,\n        aggregate=\"sum\",\n        activation=\"prelu\",\n        use_bias=True,\n        kernel_initializer=\"glorot_uniform\",\n        bias_initializer=\"zeros\",\n        kernel_regularizer=None,\n        bias_regularizer=None,\n        activity_regularizer=None,\n        kernel_constraint=None,\n        bias_constraint=None,\n        **kwargs,\n    ):\n        super().__init__(\n            aggregate=aggregate,\n            activation=None,\n            use_bias=use_bias,\n            kernel_initializer=kernel_initializer,\n            bias_initializer=bias_initializer,\n            kernel_regularizer=kernel_regularizer,\n            bias_regularizer=bias_regularizer,\n            activity_regularizer=activity_regularizer,\n            kernel_constraint=kernel_constraint,\n            bias_constraint=bias_constraint,\n            **kwargs,\n        )\n        self.channels = channels\n        self.dropout_rate = dropout\n        self.use_batch_norm = batch_norm\n        if activation == \"prelu\" or \"prelu\" in kwargs:\n            self.activation = PReLU()\n        else:\n            self.activation = activations.get(activation)\n\n    def build(self, input_shape):\n        input_dim = input_shape[0][-1]\n        self.dropout = Dropout(self.dropout_rate)\n        if self.use_batch_norm:\n            self.batch_norm = BatchNormalization()\n        self.kernel = self.add_weight(\n            shape=(input_dim, self.channels),\n            initializer=self.kernel_initializer,\n            name=\"kernel\",\n            regularizer=self.kernel_regularizer,\n            constraint=self.kernel_constraint,\n        )\n        if self.use_bias:\n            self.bias = self.add_weight(\n                shape=(self.channels,),\n                initializer=self.bias_initializer,\n                name=\"bias\",\n                regularizer=self.bias_regularizer,\n                constraint=self.bias_constraint,\n            )\n        self.built = True\n\n    def call(self, inputs, **kwargs):\n        x, a, _ = self.get_inputs(inputs)\n\n        # TODO: a = add_self_loops(a)\n\n        x = ops.matmul(x, self.kernel)\n        if self.use_bias:\n            x = ops.add(x, self.bias)\n        if self.use_batch_norm:\n            x = self.batch_norm(x)\n        x = self.dropout(x)\n        x = self.activation(x)\n\n        return self.propagate(x, a)\n\n    @property\n    def config(self):\n        config = {\n            \"channels\": self.channels,\n        }\n        if self.activation.__class__.__name__ == \"PReLU\":\n            config[\"prelu\"] = True\n\n        return config\n</code></pre> <p>             Bases: <code>MessagePassing</code></p> <p><code>k3_node.layers.GINConv</code>  Implementation of Graph Isomorphism Network (GIN) layer</p> <p>Parameters:</p> Name Type Description Default <code>channels</code> <p>The number of output channels.</p> required <code>epsilon</code> <p>The epsilon parameter for the MLP.</p> <code>None</code> <code>mlp_hidden</code> <p>A list of hidden channels for the MLP.</p> <code>None</code> <code>mlp_activation</code> <p>The activation function to use in the MLP.</p> <code>'relu'</code> <code>mlp_batchnorm</code> <p>Whether to use batch normalization in the MLP.</p> <code>True</code> <code>aggregate</code> <p>Aggregation function to use (one of 'sum', 'mean', 'max').</p> <code>'sum'</code> <code>activation</code> <p>Activation function to use.</p> <code>None</code> <code>use_bias</code> <p>Whether to add a bias to the linear transformation.</p> <code>True</code> <code>kernel_initializer</code> <p>Initializer for the <code>kernel</code> weights matrix.</p> <code>'glorot_uniform'</code> <code>bias_initializer</code> <p>Initializer for the bias vector.</p> <code>'zeros'</code> <code>kernel_regularizer</code> <p>Regularizer for the <code>kernel</code> weights matrix.</p> <code>None</code> <code>bias_regularizer</code> <p>Regularizer for the bias vector.</p> <code>None</code> <code>activity_regularizer</code> <p>Regularizer for the output.</p> <code>None</code> <code>kernel_constraint</code> <p>Constraint for the <code>kernel</code> weights matrix.</p> <code>None</code> <code>bias_constraint</code> <p>Constraint for the bias vector.</p> <code>None</code> <code>**kwargs</code> <p>Additional arguments to pass to the <code>MessagePassing</code> superclass.</p> <code>{}</code> Source code in <code>k3_node/layers/conv/gin_conv.py</code> <pre><code>class GINConv(MessagePassing):\n    \"\"\"\n    `k3_node.layers.GINConv` \n    Implementation of Graph Isomorphism Network (GIN) layer\n\n    Args:\n        channels: The number of output channels.\n        epsilon: The epsilon parameter for the MLP.\n        mlp_hidden: A list of hidden channels for the MLP.\n        mlp_activation: The activation function to use in the MLP.\n        mlp_batchnorm: Whether to use batch normalization in the MLP.\n        aggregate: Aggregation function to use (one of 'sum', 'mean', 'max').\n        activation: Activation function to use.\n        use_bias: Whether to add a bias to the linear transformation.\n        kernel_initializer: Initializer for the `kernel` weights matrix.\n        bias_initializer: Initializer for the bias vector.\n        kernel_regularizer: Regularizer for the `kernel` weights matrix.\n        bias_regularizer: Regularizer for the bias vector.\n        activity_regularizer: Regularizer for the output.\n        kernel_constraint: Constraint for the `kernel` weights matrix.\n        bias_constraint: Constraint for the bias vector.\n        **kwargs: Additional arguments to pass to the `MessagePassing` superclass.\n    \"\"\"\n    def __init__(\n        self,\n        channels,\n        epsilon=None,\n        mlp_hidden=None,\n        mlp_activation=\"relu\",\n        mlp_batchnorm=True,\n        aggregate=\"sum\",\n        activation=None,\n        use_bias=True,\n        kernel_initializer=\"glorot_uniform\",\n        bias_initializer=\"zeros\",\n        kernel_regularizer=None,\n        bias_regularizer=None,\n        activity_regularizer=None,\n        kernel_constraint=None,\n        bias_constraint=None,\n        **kwargs,\n    ):\n        super().__init__(\n            aggregate=aggregate,\n            activation=activation,\n            use_bias=use_bias,\n            kernel_initializer=kernel_initializer,\n            bias_initializer=bias_initializer,\n            kernel_regularizer=kernel_regularizer,\n            bias_regularizer=bias_regularizer,\n            activity_regularizer=activity_regularizer,\n            kernel_constraint=kernel_constraint,\n            bias_constraint=bias_constraint,\n            **kwargs,\n        )\n        self.channels = channels\n        self.epsilon = epsilon\n        self.mlp_hidden = mlp_hidden if mlp_hidden else []\n        self.mlp_activation = activations.get(mlp_activation)\n        self.mlp_batchnorm = mlp_batchnorm\n\n    def build(self, input_shape):\n        assert len(input_shape) &gt;= 2\n        layer_kwargs = dict(\n            kernel_initializer=self.kernel_initializer,\n            bias_initializer=self.bias_initializer,\n            kernel_regularizer=self.kernel_regularizer,\n            bias_regularizer=self.bias_regularizer,\n            kernel_constraint=self.kernel_constraint,\n            bias_constraint=self.bias_constraint,\n        )\n\n        self.mlp = Sequential()\n        for channels in self.mlp_hidden:\n            self.mlp.add(Dense(channels, self.mlp_activation, **layer_kwargs))\n            if self.mlp_batchnorm:\n                self.mlp.add(BatchNormalization())\n        self.mlp.add(\n            Dense(\n                self.channels, self.activation, use_bias=self.use_bias, **layer_kwargs\n            )\n        )\n\n        if self.epsilon is None:\n            self.eps = self.add_weight(shape=(1,), initializer=\"zeros\", name=\"eps\")\n        else:\n            # If epsilon is given, keep it constant\n            self.eps = ops.cast(self.epsilon, self.dtype)\n        self.one = ops.cast(1, self.dtype)\n\n        self.built = True\n\n    def call(self, inputs, **kwargs):\n        x, a, _ = self.get_inputs(inputs)\n        output = self.mlp((self.one + self.eps) * x + self.propagate(x, a))\n\n        return output\n\n    @property\n    def config(self):\n        return {\n            \"channels\": self.channels,\n            \"epsilon\": self.epsilon,\n            \"mlp_hidden\": self.mlp_hidden,\n            \"mlp_activation\": self.mlp_activation,\n            \"mlp_batchnorm\": self.mlp_batchnorm,\n        }\n</code></pre> <p>             Bases: <code>Layer</code></p> <p><code>k3_node.layers.GraphAttention</code> Implementation of Graph Attention (GAT) layer</p> <p>Parameters:</p> Name Type Description Default <code>units</code> <p>Positive integer, dimensionality of the output space.</p> required <code>attn_heads</code> <p>Positive integer, number of attention heads.</p> <code>1</code> <code>attn_heads_reduction</code> <p>{'concat', 'average'} Method for reducing attention heads.</p> <code>'concat'</code> <code>in_dropout_rate</code> <p>Dropout rate applied to the input (node features).</p> <code>0.0</code> <code>attn_dropout_rate</code> <p>Dropout rate applied to attention coefficients.</p> <code>0.0</code> <code>activation</code> <p>Activation function to use.</p> <code>'relu'</code> <code>use_bias</code> <p>Whether to add a bias to the linear transformation.</p> <code>True</code> <code>final_layer</code> <p>Deprecated, use tf.gather or GatherIndices instead.</p> <code>None</code> <code>saliency_map_support</code> <p>Whether to support saliency map calculations.</p> <code>False</code> <code>kernel_initializer</code> <p>Initializer for the <code>kernel</code> weights matrix.</p> <code>'glorot_uniform'</code> <code>kernel_regularizer</code> <p>Regularizer for the <code>kernel</code> weights matrix.</p> <code>None</code> <code>kernel_constraint</code> <p>Constraint for the <code>kernel</code> weights matrix.</p> <code>None</code> <code>bias_initializer</code> <p>Initializer for the bias vector.</p> <code>'zeros'</code> <code>bias_regularizer</code> <p>Regularizer for the bias vector.</p> <code>None</code> <code>bias_constraint</code> <p>Constraint for the bias vector.</p> <code>None</code> <code>attn_kernel_initializer</code> <p>Initializer for the attention kernel weights matrix.</p> <code>'glorot_uniform'</code> <code>attn_kernel_regularizer</code> <p>Regularizer for the attention kernel weights matrix.</p> <code>None</code> <code>attn_kernel_constraint</code> <p>Constraint for the attention kernel weights matrix.</p> <code>None</code> <code>**kwargs</code> <p>Additional arguments to pass to the <code>Layer</code> superclass.</p> <code>{}</code> Source code in <code>k3_node/layers/conv/graph_attention.py</code> <pre><code>class GraphAttention(Layer):\n    \"\"\"\n    `k3_node.layers.GraphAttention`\n    Implementation of Graph Attention (GAT) layer\n\n    Args:\n        units: Positive integer, dimensionality of the output space.\n        attn_heads: Positive integer, number of attention heads.\n        attn_heads_reduction: {'concat', 'average'} Method for reducing attention heads.\n        in_dropout_rate: Dropout rate applied to the input (node features).\n        attn_dropout_rate: Dropout rate applied to attention coefficients.\n        activation: Activation function to use.\n        use_bias: Whether to add a bias to the linear transformation.\n        final_layer: Deprecated, use tf.gather or GatherIndices instead.\n        saliency_map_support: Whether to support saliency map calculations.\n        kernel_initializer: Initializer for the `kernel` weights matrix.\n        kernel_regularizer: Regularizer for the `kernel` weights matrix.\n        kernel_constraint: Constraint for the `kernel` weights matrix.\n        bias_initializer: Initializer for the bias vector.\n        bias_regularizer: Regularizer for the bias vector.\n        bias_constraint: Constraint for the bias vector.\n        attn_kernel_initializer: Initializer for the attention kernel weights matrix.\n        attn_kernel_regularizer: Regularizer for the attention kernel weights matrix.\n        attn_kernel_constraint: Constraint for the attention kernel weights matrix.\n        **kwargs: Additional arguments to pass to the `Layer` superclass.\n    \"\"\"\n    def __init__(\n        self,\n        units,\n        attn_heads=1,\n        attn_heads_reduction=\"concat\",  # {'concat', 'average'}\n        in_dropout_rate=0.0,\n        attn_dropout_rate=0.0,\n        activation=\"relu\",\n        use_bias=True,\n        final_layer=None,\n        saliency_map_support=False,\n        kernel_initializer=\"glorot_uniform\",\n        kernel_regularizer=None,\n        kernel_constraint=None,\n        bias_initializer=\"zeros\",\n        bias_regularizer=None,\n        bias_constraint=None,\n        attn_kernel_initializer=\"glorot_uniform\",\n        attn_kernel_regularizer=None,\n        attn_kernel_constraint=None,\n        **kwargs,\n    ):\n        if attn_heads_reduction not in {\"concat\", \"average\"}:\n            raise ValueError(\n                \"{}: Possible heads reduction methods: concat, average; received {}\".format(\n                    type(self).__name__, attn_heads_reduction\n                )\n            )\n\n        self.units = units  # Number of output features (F' in the paper)\n        self.attn_heads = attn_heads  # Number of attention heads (K in the paper)\n        self.attn_heads_reduction = attn_heads_reduction  # Eq. 5 and 6 in the paper\n        self.in_dropout_rate = in_dropout_rate  # dropout rate for node features\n        self.attn_dropout_rate = attn_dropout_rate  # dropout rate for attention coefs\n        self.activation = activations.get(activation)  # Eq. 4 in the paper\n        self.use_bias = use_bias\n        if final_layer is not None:\n            raise ValueError(\n                \"'final_layer' is not longer supported, use 'tf.gather' or 'GatherIndices' separately\"\n            )\n\n        self.saliency_map_support = saliency_map_support\n        # Populated by build()\n        self.kernels = []  # Layer kernels for attention heads\n        self.biases = []  # Layer biases for attention heads\n        self.attn_kernels = []  # Attention kernels for attention heads\n\n        if attn_heads_reduction == \"concat\":\n            # Output will have shape (..., K * F')\n            self.output_dim = self.units * self.attn_heads\n        else:\n            # Output will have shape (..., F')\n            self.output_dim = self.units\n\n        self.kernel_initializer = initializers.get(kernel_initializer)\n        self.kernel_regularizer = regularizers.get(kernel_regularizer)\n        self.kernel_constraint = constraints.get(kernel_constraint)\n        self.bias_initializer = initializers.get(bias_initializer)\n        self.bias_regularizer = regularizers.get(bias_regularizer)\n        self.bias_constraint = constraints.get(bias_constraint)\n        self.attn_kernel_initializer = initializers.get(attn_kernel_initializer)\n        self.attn_kernel_regularizer = regularizers.get(attn_kernel_regularizer)\n        self.attn_kernel_constraint = constraints.get(attn_kernel_constraint)\n\n        super().__init__(**kwargs)\n\n    def build(self, input_shapes):\n        feat_shape = input_shapes[0]\n        input_dim = int(feat_shape[-1])\n\n        # Variables to support integrated gradients\n        self.delta = self.add_weight(\n            name=\"ig_delta\", shape=(), trainable=False, initializer=initializers.ones()\n        )\n        self.non_exist_edge = self.add_weight(\n            name=\"ig_non_exist_edge\",\n            shape=(),\n            trainable=False,\n            initializer=initializers.zeros(),\n        )\n\n        # Initialize weights for each attention head\n        for head in range(self.attn_heads):\n            # Layer kernel\n            kernel = self.add_weight(\n                shape=(input_dim, self.units),\n                initializer=self.kernel_initializer,\n                regularizer=self.kernel_regularizer,\n                constraint=self.kernel_constraint,\n                name=\"kernel_{}\".format(head),\n            )\n            self.kernels.append(kernel)\n\n            # # Layer bias\n            if self.use_bias:\n                bias = self.add_weight(\n                    shape=(self.units,),\n                    initializer=self.bias_initializer,\n                    regularizer=self.bias_regularizer,\n                    constraint=self.bias_constraint,\n                    name=\"bias_{}\".format(head),\n                )\n                self.biases.append(bias)\n\n            # Attention kernels\n            attn_kernel_self = self.add_weight(\n                shape=(self.units, 1),\n                initializer=self.attn_kernel_initializer,\n                regularizer=self.attn_kernel_regularizer,\n                constraint=self.attn_kernel_constraint,\n                name=\"attn_kernel_self_{}\".format(head),\n            )\n            attn_kernel_neighs = self.add_weight(\n                shape=(self.units, 1),\n                initializer=self.attn_kernel_initializer,\n                regularizer=self.attn_kernel_regularizer,\n                constraint=self.attn_kernel_constraint,\n                name=\"attn_kernel_neigh_{}\".format(head),\n            )\n            self.attn_kernels.append([attn_kernel_self, attn_kernel_neighs])\n        self.built = True\n\n    def call(self, inputs):\n        X = inputs[0]  # Node features (1 x N x F)\n        A = inputs[1]  # Adjacency matrix (1 X N x N)\n        N = ops.shape(A)[-1]\n\n        assert len(ops.shape(A)) == 2, f\"Adjacency matrix A should be 2-D\"\n\n        outputs = []\n        for head in range(self.attn_heads):\n            kernel = self.kernels[head]  # W in the paper (F x F')\n            attention_kernel = self.attn_kernels[\n                head\n            ]  # Attention kernel a in the paper (2F' x 1)\n\n            # Compute inputs to attention network\n\n            features = ops.dot(X, kernel)  # (N x F')\n\n            # Compute feature combinations\n            # Note: [[a_1], [a_2]]^T [[Wh_i], [Wh_2]] = [a_1]^T [Wh_i] + [a_2]^T [Wh_j]\n            attn_for_self = ops.dot(\n                features, attention_kernel[0]\n            )  # (N x 1), [a_1]^T [Wh_i]\n            attn_for_neighs = ops.dot(\n                features, attention_kernel[1]\n            )  # (N x 1), [a_2]^T [Wh_j]\n\n            # Attention head a(Wh_i, Wh_j) = a^T [[Wh_i], [Wh_j]]\n            dense = attn_for_self + ops.transpose(\n                attn_for_neighs\n            )  # (N x N) via broadcasting\n\n            dense = LeakyReLU(0.2)(dense)\n\n            if not self.saliency_map_support:\n                mask = -10e9 * (1.0 - A)\n                dense += mask\n                dense = ops.softmax(dense)  # (N x N), Eq. 3 of the paper\n\n            else:\n                # dense = dense - tf.reduce_max(dense)\n                # GAT with support for saliency calculations\n                W = (self.delta * A) * ops.exp(\n                    dense - ops.max(dense, axis=1, keepdims=True)\n                ) * (1 - self.non_exist_edge) + self.non_exist_edge * (\n                    A + self.delta * (ops.ones((N, N)) - A) + ops.eye(N)\n                ) * ops.exp(\n                    dense - ops.max(dense, axis=1, keepdims=True)\n                )\n                dense = W / ops.sum(W, axis=1, keepdims=True)\n\n            # Apply dropout to features and attention coefficients\n            dropout_feat = Dropout(self.in_dropout_rate)(features)  # (N x F')\n            dropout_attn = Dropout(self.attn_dropout_rate)(dense)  # (N x N)\n\n            # Linear combination with neighbors' features [YT: see Eq. 4]\n            node_features = ops.dot(dropout_attn, dropout_feat)  # (N x F')\n\n            if self.use_bias:\n                node_features = ops.add(node_features, self.biases[head])\n\n            # Add output of attention head to final output\n            outputs.append(node_features)\n\n        # Aggregate the heads' output according to the reduction method\n        if self.attn_heads_reduction == \"concat\":\n            output = ops.concatenate(outputs, axis=1)  # (N x KF')\n        else:\n            output = ops.mean(ops.stack(outputs), axis=0)  # N x F')\n\n        output = self.activation(output)\n\n        return output\n</code></pre> <p>             Bases: <code>Layer</code></p> <p><code>k3_node.layers.conv.MessagePassing</code> Base class for message passing layers.</p> <p>Parameters:</p> Name Type Description Default <code>aggregate</code> <p>Aggregation function to use (one of 'sum', 'mean', 'max').</p> <code>'sum'</code> <code>**kwargs</code> <p>Additional arguments to pass to the <code>Layer</code> superclass.</p> <code>{}</code> Source code in <code>k3_node/layers/conv/message_passing.py</code> <pre><code>class MessagePassing(layers.Layer):\n    \"\"\"\n    `k3_node.layers.conv.MessagePassing`\n    Base class for message passing layers.\n\n    Args:\n        aggregate: Aggregation function to use (one of 'sum', 'mean', 'max').\n        **kwargs: Additional arguments to pass to the `Layer` superclass.\n    \"\"\"\n    def __init__(self, aggregate=\"sum\", **kwargs):\n        super().__init__(**{k: v for k, v in kwargs.items() if is_keras_kwarg(k)})\n        self.kwargs_keys = []\n        for key in kwargs:\n            if is_layer_kwarg(key):\n                attr = kwargs[key]\n                attr = deserialize_kwarg(key, attr)\n                self.kwargs_keys.append(key)\n                setattr(self, key, attr)\n\n        self.msg_signature = inspect.signature(self.message).parameters\n        self.agg_signature = inspect.signature(self.aggregate).parameters\n        self.upd_signature = inspect.signature(self.update).parameters\n        self.agg = deserialize_scatter(aggregate)\n\n    def call(self, inputs, **kwargs):\n        x, a, e = self.get_inputs(inputs)\n        return self.propagate(x, a, e)\n\n    def build(self, input_shape):\n        self.built = True\n\n    def propagate(self, x, a, e=None, **kwargs):\n        self.n_nodes = ops.shape(x)[-2]\n        self.index_sources, self.index_targets = get_source_target(a)\n\n        msg_kwargs = self.get_kwargs(x, a, e, self.msg_signature, kwargs)\n        messages = self.message(x, **msg_kwargs)\n\n        agg_kwargs = self.get_kwargs(x, a, e, self.agg_signature, kwargs)\n        embeddings = self.aggregate(messages, **agg_kwargs)\n\n        upd_kwargs = self.get_kwargs(x, a, e, self.upd_signature, kwargs)\n        output = self.update(embeddings, **upd_kwargs)\n        return output\n\n    def message(self, x, **kwargs):\n        return self.get_sources(x)\n\n    def aggregate(self, messages, **kwargs):\n        return self.agg(messages, self.index_targets, self.n_nodes)\n\n    def update(self, embeddings, **kwargs):\n        return embeddings\n\n    def get_targets(self, x):\n        return ops.take(x, self.index_targets, axis=-2)\n\n    def get_sources(self, x):\n        return ops.take(x, self.index_sources, axis=-2)\n\n    def get_kwargs(self, x, a, e, signature, kwargs):\n        output = {}\n        for k in signature.keys():\n            if signature[k].default is inspect.Parameter.empty or k == \"kwargs\":\n                pass\n            elif k == \"x\":\n                output[k] = x\n            elif k == \"a\":\n                output[k] = a\n            elif k == \"e\":\n                output[k] = e\n            elif k in kwargs:\n                output[k] = kwargs[k]\n            else:\n                raise ValueError(\"Missing key {} for signature {}\".format(k, signature))\n\n        return output\n\n    @staticmethod\n    def get_inputs(inputs):\n        if len(inputs) == 3:\n            x, a, e = inputs\n            assert len(ops.shape(e)) in (2, 3), \"E must have rank 2 or 3\"\n        elif len(inputs) == 2:\n            x, a = inputs\n            e = None\n        else:\n            raise ValueError(\n                \"Expected 2 or 3 inputs tensors (X, A, E), got {}.\".format(len(inputs))\n            )\n        assert len(ops.shape(a)) == 2, \"A must have rank 2\"\n\n        return x, a, e\n\n    @staticmethod\n    def preprocess(a):\n        return a\n\n    def get_config(self):\n        mp_config = {\"aggregate\": serialize_scatter(self.agg)}\n        keras_config = {}\n        for key in self.kwargs_keys:\n            keras_config[key] = serialize_kwarg(key, getattr(self, key))\n        base_config = super().get_config()\n\n        return {**base_config, **keras_config, **mp_config, **self.config}\n\n    @property\n    def config(self):\n        return {}\n</code></pre> <p>             Bases: <code>Layer</code></p> <p><code>k3_node.layers.PPNPPropagation</code> Implementation of PPNP layer</p> <p>Parameters:</p> Name Type Description Default <code>units</code> <p>Positive integer, dimensionality of the output space.</p> required <code>final_layer</code> <p>Deprecated, use tf.gather or GatherIndices instead.</p> <code>None</code> <code>input_dim</code> <p>Deprecated, use <code>keras.layers.Input</code> with <code>input_shape</code> instead.</p> <code>None</code> <code>**kwargs</code> <p>Additional arguments to pass to the <code>Layer</code> superclass.</p> <code>{}</code> Source code in <code>k3_node/layers/conv/ppnp.py</code> <pre><code>class PPNPPropagation(Layer):\n    \"\"\"\n    `k3_node.layers.PPNPPropagation`\n    Implementation of PPNP layer\n\n    Args:\n        units: Positive integer, dimensionality of the output space.\n        final_layer: Deprecated, use tf.gather or GatherIndices instead.\n        input_dim: Deprecated, use `keras.layers.Input` with `input_shape` instead.\n        **kwargs: Additional arguments to pass to the `Layer` superclass. \n    \"\"\"\n    def __init__(self, units, final_layer=None, input_dim=None, **kwargs):\n        if \"input_shape\" not in kwargs and input_dim is not None:\n            kwargs[\"input_shape\"] = (input_dim,)\n\n        super().__init__(**kwargs)\n\n        self.units = units\n        if final_layer is not None:\n            raise ValueError(\"'final_layer' is not longer supported.\")\n\n    def get_config(self):\n        config = {\"units\": self.units}\n\n        base_config = super().get_config()\n        return {**base_config, **config}\n\n    def compute_output_shape(self, input_shapes):\n        feature_shape, *As_shapes = input_shapes\n\n        batch_dim = feature_shape[0]\n        out_dim = feature_shape[1]\n\n        return batch_dim, out_dim, self.units\n\n    def build(self, input_shapes):\n        self.built = True\n\n    def call(self, inputs):\n        x, a = inputs\n        n_nodes, _ = ops.shape(x)\n        output = ops.dot(x, a)\n        return output\n</code></pre> <p>             Bases: <code>Layer</code></p> <p><code>k3_node.layers.SAGEConv</code> Implementation of GraphSAGE layer</p> <p>Parameters:</p> Name Type Description Default <code>out_channels</code> <p>The number of output channels.</p> required <code>normalize</code> <p>Whether to normalize the output.</p> <code>False</code> <code>bias</code> <p>Whether to add a bias to the linear transformation.</p> <code>True</code> Source code in <code>k3_node/layers/conv/sage_conv.py</code> <pre><code>class SAGEConv(layers.Layer):\n    \"\"\"\n    `k3_node.layers.SAGEConv`\n    Implementation of GraphSAGE layer\n\n    Args:\n        out_channels: The number of output channels.\n        normalize: Whether to normalize the output.\n        bias: Whether to add a bias to the linear transformation.\n    \"\"\"\n    def __init__(self, out_channels, normalize=False, bias=True):\n        super().__init__()\n        self.out_channels = out_channels\n        self.normalize = normalize\n\n        self.lin_rel = layers.Dense(out_channels, use_bias=False)\n        self.lin_root = layers.Dense(out_channels, use_bias=bias)\n\n    def call(self, x, adj, mask=None):\n        # x = ops.expand_dims(x, axis=0) if len(ops.shape(x)) == 2 else x\n        # adj = ops.expand_dims(adj, axis=0) if len(ops.shape(adj)) == 2 else adj\n\n        out = ops.matmul(adj, x)\n        out = out / ops.clip(\n            ops.sum(adj, axis=-1, keepdims=True), x_min=1.0, x_max=float(\"inf\")\n        )\n        out = self.lin_rel(out) + self.lin_root(x)\n\n        if self.normalize:\n            out = keras.utils.normalize(out, axis=-1)\n        if mask is not None:\n            mask = ops.expand_dims(mask, axis=-1)\n            out = ops.multiply(out, mask)\n\n        return out\n</code></pre>"},{"location":"layers/","title":"k3_node.layers","text":"<p><code>k3_node.layers</code> module provides access to various layers for building  graph neural networks.</p>"},{"location":"layers/#attention-layers","title":"Attention Layers","text":""},{"location":"layers/#aggregation-layers","title":"Aggregation Layers","text":""},{"location":"layers/#convolution-layers","title":"Convolution Layers","text":""},{"location":"norm_layers/","title":"Normalization Layers","text":"<p>             Bases: <code>Layer</code></p> <p><code>k3_node.layers.MeanSubtractionNorm</code> Implementation of Mean Subtraction Norm layer</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <p>Additional arguments to pass to the <code>Layer</code> superclass.</p> <code>{}</code> Call Arguments <p><code>x</code> Input tensor.</p> Call Returns <p>Output tensor of the same shape as <code>x</code>.</p> <pre><code>import numpy as np\nfrom k3_node.layers import MeanSubtractionNorm\nx = np.array([[1, 2, 3], [4, 5, 6]], dtype=np.float32)\nlayer = MeanSubtractionNorm()\nlayer(x).numpy()\n</code></pre> Source code in <code>k3_node/layers/norm/mean_subtraction_norm.py</code> <pre><code>class MeanSubtractionNorm(layers.Layer):\n    \"\"\"\n    `k3_node.layers.MeanSubtractionNorm`\n    Implementation of Mean Subtraction Norm layer\n\n    Args:\n        **kwargs: Additional arguments to pass to the `Layer` superclass.\n\n    Call Arguments:\n        `x` Input tensor.\n\n    Call Returns:\n        Output tensor of the same shape as `x`.\n    ```python\n    import numpy as np\n    from k3_node.layers import MeanSubtractionNorm\n    x = np.array([[1, 2, 3], [4, 5, 6]], dtype=np.float32)\n    layer = MeanSubtractionNorm()\n    layer(x).numpy()\n    ```\n    \"\"\"\n    def __init__(self, **kwargs):\n        super(MeanSubtractionNorm, self).__init__(**kwargs)\n\n    def call(self, x):\n        return x - ops.mean(x, axis=0, keepdims=True)\n</code></pre>"}]}